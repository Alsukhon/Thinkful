{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time has come to learn your first real model: Naive Bayes. The reason we're introducing this model first is because, actually, we already introduced everything you need to know a while ago. You should be familiar with __Bayes Rule__, which we covered in the fundamentals course (if you've forgotten don't worry, we'll cover it again briefly here). Naive Bayes is simply modeling and prediction based around this theorem.\n",
    "\n",
    "Let's approach this by thinking about Naive Bayes in terms of the two words in its name: Naive and Bayes.\n",
    "\n",
    "## Bayes\n",
    "\n",
    "Let's discuss Bayes first, since that's the core of the model. Bayes Theorem covers the probabilistic relationship between multiple variables, and specifically allows us to define one conditional in terms of the underlying probabilities and the inverse condition. Specifically, it can be defined as:\n",
    "\n",
    "$$P(y|x) = P(y)P(x|y)/P(x)$$\n",
    "\n",
    "In English this reads as \"the probability of y given x equals the probability of y times the probability of x given y divided by the probability of x.\"\n",
    "\n",
    "This theorem can be extended to when x is a vector (containing the multiple x variables used as inputs for the model) to:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = P(y)P(x_1,...,x_n|y)/P(x_1,...,x_n)$$\n",
    "\n",
    "This explains the relationship of an outcome to a vector of conditions rather than to a single other event. Recall that this can be read as the probability of y, in the case of our model the categorical outcome we’re interested in, given a set of observations is equal to the probability of that set of observations given y divided by the probability of that set of outcomes.\n",
    "\n",
    "We'll return to this probability later to define our model.\n",
    "\n",
    "## Naive\n",
    "\n",
    "The other part of Naive Bayes is of course Naive. In this setting Naive refers to the assumption that any pair of variables in the conditional vector (the x variables) are independent from each other.\n",
    "\n",
    "That independence does something really important to the vectorized Bayes Rule equation (the second one from above). It allows us to break that large $P(x_1,...,x_n|y)$ into the product of each individual condition. Written out it would be something like:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = P(y)*P(x_1|y)*...*P(x_n|y)/P(x_1,...,x_n)$$\n",
    "\n",
    "We can even simplify further because for any observation we are attempting to predict, the x-vector will be constant, so that part of the probability simplifies out leaving:\n",
    "\n",
    "$$P(y|x_1,...,x_n) \\approx P(y)*P(x_1|y)*...*P(x_n|y)$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\\hat{y} = argmax_y(P(y)\\prod_{i=1}^nP(x_i|y))$$\n",
    "\n",
    "This states that our estimator of y is the maximum over y of the $P(y)*\\prod_{i=1}^nP(x_i|y)$.\n",
    "\n",
    "This is the basis for Naive Bayes as a model. As you can tell from this formula, Naive Bayes returns the y value that maximizes the following argument. This means it returns a single value, or category. \n",
    "\n",
    "Returning to the fact that our estimate is the y that maximizes the argument, this is because Naive Bayes is used as a classifier. We are interested in which y value is most likely to have given the observed set of x variables based on their Bayesian probabilities. There are ways to jigger the rule into returning probabilities, but these are generally NOT very accurate and not to be used. It is said that this is a good classifier but not a good estimator.\n",
    "\n",
    "\n",
    "## Simple is Sometimes better\n",
    "\n",
    "Now, we made some huge assumptions here to get to this predictor. The assumption of independence between each pair is hugely important and rarely totally accurate. The columns of your data frame are typically not independent of each other. \n",
    "\n",
    "However, Naive Bayes is still used in the real world.\n",
    "\n",
    "There are a few different reasons for that. Firstly, it is delightfully simple. It is easy to understand both how it operates and what it's doing. More importantly it is incredibly fast. That speed can occasionally be very useful from a practical perspective. It also relies on probabilities, which are based on counts, so you can actually train the classifier with more data than could fit into memory at one time (and scikit-learn even has an option to do this). That count reliance also contributes to its computational simplicity.\n",
    "\n",
    "There are also specific situations where Naive Bayes has been known to perform reasonably well. This is most common in sentiment classification, a branch of machine learning that is designed to focus on trying to classify textual samples according to sentiment. Practically it is very good for spam filtering or telling if comments are positive or negative.\n",
    "\n",
    "We'll make a Bayesian spam filter later in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We’ve introduced the core concept of a Naive Bayes Classifier, but there are still some details to sort out. In this section we’ll cover a bit of the decision process that goes into running the model, ways to improve performance, and some of the risks and downsides of using Naive Bayes.\n",
    "\n",
    "## Types of Naive Bayes\n",
    "\n",
    "When actually running a Naive Bayes classifier, you will have to make one more assumption. That assumption is around the distribution of P(x<sub>i</sub>|y).\n",
    "\n",
    "There are three main classifiers: Bernoulli, Multinomial, and Gaussian Naive Bayes. We’ve covered these distributions briefly in the fundamentals course. Each classifier assumes that the distribution of the conditional (the aforementioned P(x<sub>i</sub>|y))  is the given distribution.\n",
    "\n",
    "Now these distributions have limitations. A binomial only takes two possible values. A multinomial has discrete outcomes, and a Gaussian (also known as \"normal\") takes values along the continuous normal distribution.\n",
    "\n",
    "What this means is that choosing which kind of classifier you want to use depends on the distribution of your outcome variable. Choose the distribution that best fits your data. It should be pretty obvious, for instance, if it is best modeled using Bernoulli (the variable will be binary) and so on. \n",
    "\n",
    "If you’re interested in reading further about these types of Bayes classifiers, you can check out the [scikit-learn documentation](http://scikit-learn.org/stable/modules/naive_bayes.html).\n",
    "\n",
    "## Improving Performance\n",
    "\n",
    "In running this kind of classifier, there are many things you can do to improve the performance of your model.\n",
    "\n",
    "The first and most important thing will be, as is often the case, feature engineering. This is particularly true in text based problems (which we’ll cover at greater length in the NLP section later in the course). Here it is largely up to the creativity and knowledge of the one building the model to draw out the right features.\n",
    "\n",
    "In Naive Bayes, feature selection can also be important. Because features are equally weighted, heavily correlated features can lead to doubling the impact of what is essentially a single feature. Remember, you’re making the assumption that every pair of variables is independent of each other. The more removed from that assumption reality is, the more problems you may run into.\n",
    "\n",
    "## Downsides of Naive Bayes\n",
    "\n",
    "The first and most obvious downside of Naive Bayes is that assumption of independence. That is a double edged sword because not only is it a condition you’ll often fail to have (even when the model works well), but it also means that any time two variables affect the outcome most in concert your model will fail to see it. This kind of effect is called interaction, and occurs when any two features create a different effect when they both have a specific value than they would as independent occurrences. In Naive Bayes any such interaction is lost.\n",
    "\n",
    "Also, Naive Bayes can only predict the outcome of categories it has seen before. This applies both to the outcome and the inputs. If you have new x-values in test, the model will default to ignoring that specific outcome. Like all classifiers it cannot predict a class it hasn’t seen. The way Naive Bayes handles partial data, however, does have the benefit of being indifferent to missing datapoints. Those missing datapoints simply get ignored, drawing what information it can from the other variables of that observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
