{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Decision trees are great, and their easily represented set of rules is a powerful feature for modeling and even more so for conveying that model to a more general audience. But their high variance and propensity to overfit are _serious problems_. Luckily there is a way we can handle both of those issues and create an even more powerful kind of model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Random Forest?\n",
    "\n",
    "What if instead of making one decision tree you made several. As many as you wanted, really. A whole forest. Then each tree in the forest got a vote on the outcome for a given observation. Then you'd have a new model type: **Random Forest**. Random forests have become an incredibly popular technique for data scientists because it tends to be a top performer in a huge number of circumstances, with low variance and high accuracy.\n",
    "\n",
    "Much like Decision Trees, Random Forest can be used for both classification and regression problems. The main difference is how the votes are aggregated. As a classifier the most popular outcome (the mode) is returned. As a regression it is typically the average or mean that is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "When building a Random Forest you get to set parameters for both the tree and the forest. So for the tree you have the same parameters as before: you can set the depth of the tree and the number of features used in each rule or split. You can also specify how the tree is built, with using information gain and entropy like we did before, or other methods like [Gini impurity](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria).\n",
    "\n",
    "You also get to control the number of estimators you want to generate, or the number of trees in the forest. Here you have a tradeoff between how much variance you can explain and the computational complexity. This is pretty easily tuneable. As you increase the number of trees in the forest the accuracy should converge as eventually the additional learning from another tree approaches zero. There isn't an infinite amount of information to learn, and at some point the trees have learned all they can. So when you have acceptable variance in accuracy you can stop adding trees. This becomes worthwhile when you're dealing with large datasets with many variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Bagging and Random Subspace\n",
    "\n",
    "Now, Random Forest models don't just create a ton of trees using the same data again and again and again. Instead they use bagging and random subspace to generate trees that are different. Without this, the trees could be incredibly similar (even identical), leading to correlation between trees and vulnerability to bias in the trees from some highly predictive features dominating every tree, creating a series of very similar trees with very similar, and potentially biased, predictions.\n",
    "\n",
    "Firstly, Random Forests use **bagging**. Each tree selects a subset of observations with replacement to build the training set. Replacement here means it can simply choose the same observation multiple times, which is only really a problem when there are few observations. It puts the observation \"back in the bag\", if you will, where it can be pulled and chosen again.\n",
    "\n",
    "Random forests also typically use a random subset of features for each split. This means for each time it has to perform a split or generate a rule, it is only looking at the **random subspace** created by a random subset of _some_ of the features as possibilities to generate that rule. This will help avoid the aforementioned correlation problem because the trees will not be built with the same available features at every point. As a general rule, for a dataset with x features $\\sqrt{x}$ features are used for classifiers and $x/3$ for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Random Forest\n",
    "\n",
    "The biggest advantage of Random Forest is its tendency to be a very strong performer. It is reasonably accurate in a myriad of situations from regression to classification. Some people [really really love it](https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883#.rq8akkff1). However it is not without its disadvantages.\n",
    "\n",
    "Firstly in either classification or regression it will not predict outside of sample, meaning it will only return values that are within a range it has seen before. Random Forests can also get rather large and slow if you let them grow too wildly.\n",
    "\n",
    "The biggest disadvantage, however, is the lack of transparency in the process. Random Forest is often referred to as a \"**black box**\" model, meaning it will give you an output but very little insight into how it got there. You'll run into a few more of these throughout the course.\n",
    "\n",
    "Black box models often make the more statistically minded data scientists nervous. You don't get much insight into the process. You can't see the rules it's really applying or what variables it's prioritizing or how. You don't see any of the internal process or get to look \"inside the box\". Therefore you also can't represent that process in a simple visual form or learn about the underlying process. You have to trust in the algorithm building the trees and the lack of variance from a large number of them being generated. It usually works out pretty well, and you can of course evaluate the model via other methods to validate your conclusions.\n",
    "\n",
    "In the next section we'll walk through an example of the Random Forest Classifier."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "96px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
