{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is optional content!  For a deep dive into testing for normality and what happens to that 5% chance of a false positive when you run lots and lots of tests, read on.  You do not need to master this content to succeed in the course.\n",
    "\n",
    "## Testing for normality\n",
    "This is the formula for the Shapiro-Wilk test statistic, *W*.\n",
    "\n",
    "\\begin{align} \n",
    "W=\\frac { (\\sum_{i=1}^n a_ix_{(i)} )^2}{ \\sum_{i=1}^n (x_i-\\bar{x})^2}\n",
    "\\end{align}\n",
    "\n",
    "$x_{(i)}$ is the *i*th smallest number in the sample.  \n",
    "$a_i$ is a constant determined by the sample size and acts as a scaling value.  \n",
    "$\\bar{x}$ is the sample mean.  \n",
    "$x_i$ is the *i*th observation in the sample.\n",
    "\n",
    "The equation computes the ratio between the value that would be expected for a normally-distributed sample of that size containing that much information (the numerator of the fraction above) and the actual sum of the differences between each of the values in the variable and the sample mean (the denominator).  Values close to 1 indicate that the distribution is similar to a normal distribution.  The smaller the W statistic becomes, the more divergence there is between the distribution of the data and the normal distribution.  \n",
    "\n",
    "Note that when comparing groups, the distribution of each *group* must be normal.  \n",
    "\n",
    "The Shapiro-Wilk test (and all other tests of non-normality) come with an important caveat: they are very sensitive to sample size. For small samples (50, others >2000 or more) the test will detect even very small and unimportant deviations from non-normality. Statistical tests of normality should always be accompanied by visualizations.\n",
    "\n",
    "## Multiple Testing Correction: Tukey's Honest Significant Differences (HSD) Test\n",
    "\n",
    "Instead of running many pairs of t-tests to find out which of the three roller coaster materials was the odd one out, we could run a Tukey's HSD test.  Unlike a t-test, Tukey's HSD test does pairwise tests that use a variability estimate based on variability from all the groups combined (the denominator from the F-test mentioned in the previous checkpoint) rather than variability from only the two groups being tested.  \n",
    "\n",
    "\\begin{equation}\n",
    "Q=\\frac{M_i-M_j}{\\sqrt{MSE/n}}\n",
    "\\end{equation}\n",
    "\n",
    "Where $MSE={\\sum\\sum(Y_{ij}-\\bar{Y}_j)^2}/{(N-a)}$ from the denominator of the F-test.\n",
    "\n",
    "In addition, when calculating the probability of getting this ratio, the test statistic, *Q* will be evaluated in light of a modified probability distribution that takes into account the number of means being tested across all pairwise tests. \n",
    "\n",
    "Running Tukey's HSD using Python's `statsmodels` package will get us a table with the differences between each pair of means, the upper and lower bounds of that difference estimate, and whether we should reject the null hypothesis that each pair of groups is not different.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multiple Comparison of Means - Tukey HSD,FWER=0.05</caption>\n",
       "<tr>\n",
       "  <th>group1</th>  <th>group2</th> <th>meandiff</th>  <th>lower</th>   <th>upper</th>  <th>reject</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Plastic</td>  <td>Steel</td>  <td>9.3698</td>  <td>2.8923</td>  <td>15.8474</td>  <td>True</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Plastic</td>  <td>Wood</td>   <td>17.6466</td> <td>11.1691</td> <td>24.1241</td>  <td>True</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>Steel</td>   <td>Wood</td>   <td>8.2768</td>  <td>1.7992</td>  <td>14.7543</td>  <td>True</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coaster_heights = pd.DataFrame()\n",
    "\n",
    "steel_heights = [\n",
    "    18.5, 14, 30.2, 25.2024, 15, 16, 13.5, 30, 20, 17, 13.716, 8.5, 16.1, 18,\n",
    "    41, 30.3, 32.004, 28.004, 30.48, 34\n",
    "    ]\n",
    "\n",
    "wood_heights = [\n",
    "    38.70, 46, 27.8, 43.52, 33.77, 29.26, 16.764, 45, 48.1, 16.764, 24.384,\n",
    "    24.5, 40, 35.96, 22.24, 21.33, 27.73, 23.46, 21.64, 30.12\n",
    "    ]\n",
    "\n",
    "plastic_heights = [\n",
    "    9, 8.2, 12, 21, 6.3, 11.7, 19.44, 4.75, 13, 18, 15.5, 15.6, 10, 11.77, 29,\n",
    "    5, 3.2, 14.75, 18.2, 17.7\n",
    "    ]\n",
    "\n",
    "coaster_heights['Steel'] = steel_heights\n",
    "coaster_heights['Wood'] = wood_heights\n",
    "coaster_heights['Plastic'] = plastic_heights\n",
    "\n",
    "heights=np.asarray(\n",
    "    coaster_heights['Steel'].tolist() +\n",
    "    coaster_heights['Wood'].tolist() +\n",
    "    coaster_heights['Plastic'].tolist())\n",
    "\n",
    "materials = np.array(['Steel', 'Wood','Plastic'])\n",
    "materials = np.repeat(materials, 20)\n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=heights,      # Data\n",
    "                          groups=materials,   # Groups\n",
    "                          alpha=0.05)         # Significance level\n",
    "\n",
    "tukey.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Testing Correction\n",
    "\n",
    "You may be wondering why we went through all this trouble, when we could have just done a bunch of t-tests comparing the groups.  The reason for doing an ANOVA first, and following-up with post-hoc tests if the F-test suggests differences, is to prevent false positive results.  \n",
    "\n",
    "As you've read several times by now, data scientists often use a probability threshold of 5% (p = .05) to determine whether groups we are comparing in our sample are meaningfully different in the population (though the use of this threshold, or even the existence of a threshold at all, is widely debated: [See this article for a deep dive into the issue](\"http://www.stats.org.uk/statistical-inference/Johnson1999.pdf\")).   One way of interpreting that threshold is that if there were no real difference between the groups in the population, we would see differences like those in our sample less than 5% of the time.  To put it another way, this threshold means we have a 1 in 20 chance of a false positive, of claiming there is a real difference when in fact, there is not.  \n",
    "\n",
    "This 1 in 20 chance, however, only holds when we have done exactly one statistical test on the data.  If we perform two statistical tests on the same data, the chances of getting a false positive on at least one of them are now 1 in 10: $\\frac1{20}+\\frac1{20}$.  If we were to perform 200 tests on the same data, it is likely that up to 10 of our tests are false positives.  \n",
    "\n",
    "As data scientists, we want to be confident that our conclusions accurately reflect the population.  There are various  **multiple test correction** methods that can be used to keep chances of a false positive below the 5% threshold.  One is Tukey's HSD test, which uses information about the overall sample variance and the number of groups being compared to \"raise the bar\" on how large a group difference must be before it passes the 5% probability threshold.  \n",
    "\n",
    "There are many methods, some specific to certain types of analysis and others representing more general approaches suitable for many different statistical goals.  [This article in Nature Biotechnology](https://www.nature.com/articles/nbt1209-1135) has a good review of decision-points involved in selecting a correctional approach.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
