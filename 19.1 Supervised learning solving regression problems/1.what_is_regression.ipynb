{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning deals with two broad types of problems: **regression** and **classification**. In a classification problem, the target variable can take a limited (and typically, very few) number of values, which are called categories or classes. More often than not, there is not an ordered relationship between categories. As an example, think about how Facebook tags people who appear in images. Tagging a person in an image is a classification task as the number of available tags is limited. \n",
    "\n",
    "While it’s great to be able to solve classification problems, there’s a whole class of problems that require a different approach — for example, predicting Walmart’s gross revenue at the end of a year. In that case, we’re not trying to *classify*, rather, we’re trying to *quantify*. And that means we’re interested in an outcome that in theory can take on an infinite number of values.\n",
    "\n",
    "You might be tempted to think that you can handle this by treating each possible numeric value as a single category. But following this through to its logical conclusion, we would need training data for each point in an infinite set of points, which is simply impossible.  \n",
    "\n",
    "When working with target variables that can take on a large, potentially infinite, range of values, linear regression is one of your best friends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression vs classification\n",
    "\n",
    "Let’s say a bit more about how regression differs from classification. \n",
    "\n",
    "For a classification problem, the target variable is **categorical**. This means that the variable only takes discrete values from within a specified set. Simple versions of that set could look something like {yes, no} or {heads, tails}. The outcomes could also contain more than two values such as {high, medium, low} or {buy, rent, no purchase}. Almost anything that can be discretely counted and labeled can be considered a categorical variable. \n",
    "\n",
    "In contrast, regression problems have a **continuous** outcome variable. For example, you may want to build a model that predicts the next day closing price of Apple’s stock on the Nasdaq. Obviously, Apple’s stock price is not a categorical variable. Although it is somewhat bounded from above and below, it still has an infinite number of possible values.\n",
    "\n",
    "And then there are the edge cases. Imagine the target variable you're interested in is the number of watches owned {0, 1, 2, 3, 4, 5,….,15} and you've never seen someone with more than 15. Should you use regression or classification? The fact is you could use either. If you use classification, then for each observation you're only going to be able to evaluate the likelihood of each given value. Each outcome level will be treated discretely, and the relationship between them is not predefined. But if you use regression, you're going to find their most likely place along a continuous line, without necessarily even having bounds at zero or 15. Each approach has its advantages, and you should choose according the the output you require.\n",
    "\n",
    "Regression tasks are prevalent in data science. Examples include stock price prediction, revenue prediction, weather forecasting, and demand prediction. In these kinds of problem areas, we’re often interested in outcome variables that are continuous or have so many values so that it’s easiest to treat them as continuous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating a regression problem \n",
    "\n",
    "Linear regression models the relationship between one or more observed features and some continuous target variable. Based on observed data, we try to derive a function that describes the target as a function of the feature set. This function produces a line where we can look up feature values and find predicted target values. \n",
    "\n",
    "Let’s illustrate this with an example. For a fictional football (*and in this case we mean what the world outside the USA means by football, aka soccer!*) league called *Fantastic Football League*, we collected some data about the average number of goals scored with respect to the number of the shots in a match. The data is presented in the chart below:\n",
    "\n",
    "\n",
    "![chart depicting goals scored as function of goals attempted](assets/fantastic_football_linear_1.png)\n",
    "\n",
    " \n",
    "Just eyeballing this data, we can see there’s a positive correlation between number of shots and number of goals scored in the observed cases.  This scatterplot gives us the intuition that there’s an underlying relationship between number of shots taken and number of goals scored, but it doesn’t quite allow us to make a prediction about unobserved cases (for instance, when a team scores an average number of 6.634 goals per match). \n",
    "\n",
    "What we need is a way to extract a model — which is to say a mathematical description — of the underlying relationship between feature and target. Conceptually, we want to find a line we can draw through the scatterplot of our known data that represents the relationship between the feature and the target reasonably well, and that also does a good job at predicting values that were not directly observed in our data. In short, we want something like the chart below:\n",
    "\n",
    "![another chart depicting goals scored as function of goals attempted](assets/fantastic_football_linear_2.png)\n",
    "\n",
    "Here, we have our original scatterplot of data, with a positively sloping line (<font color=blue>the blue one</font>) that our observed cases appear to cluster around. To answer the question: *What are the expected number of goals when a team takes 8.39 shots on average?* We look up what the Y-value (average number of goals) is when number of shots is 8.39 (eyeballing it, it looks like Y is at about .7).  Again, we don’t have an observed case of 8.39 shots, but our model makes a prediction about what we would expect number of goals to be, based on the cases it has seen.\n",
    "\n",
    "This example is simple and stripped down, but hopefully it starts to give you a sense of the power of regression models. If you can build models that uncover underlying linear relationships in data, you gain the power of prediction. This kind of knowledge can truly drive business value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does linear regression work?\n",
    "\n",
    "Let’s start with the demonstration of how linear regression helps us to find the relationship we want using our single-feature, single-target football data. With linear regression, we plot each observation on an x- and y-axis and then draw a line that best fits the observed cases.  As the chart below demonstrates, the fitted <font color=blue>blue line</font> represents a linear relationship between the number of shots and the average number of goals scored:\n",
    "\n",
    "\n",
    "![yet another chart depicting goals scored as function of goals attempted](assets/fantastic_football_linear_3.png)\n",
    "\n",
    "Actually, <font color=blue>the blue line</font> just represents a linear function, which in turn represents the relationship between the number of shots and the average number of goals scored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building regression models as an iterative process\n",
    "\n",
    "In the remaining checkpoints in this module, we’ll get into the nitty-gritty details of the process for creating linear regression models, but here at the outset, it will be helpful to give you a high-level view of the main steps.\n",
    "\n",
    "\n",
    "### 1. Designing a model:  \n",
    "\n",
    "The first step is to choose an initial set of features that you believe have a relationship with the target variable. The football example includes only one feature. However, in a more realistic dataset, you will have more than one feature that can contribute to your model. Choosing the initial feature set stems from the feature engineering and exploratory analysis that you typically do before the model selection.\n",
    "\n",
    "Assuming we have two features ($x$ and $z$) and one target ($y$), our initial model can be expressed in the following form:\n",
    "\n",
    "$$ y = \\alpha + \\beta x + \\theta z $$\n",
    "  \n",
    "In the formula above, $\\alpha$ is called the **bias term**, $\\beta$ and $\\theta$ are called the **coefficients**. \n",
    "\n",
    "\n",
    "### 2. Estimating the coefficients: \n",
    "\n",
    "The next step is to find the optimal parameters of your model. This is the same thing as to draw <font color=blue>the blue line</font> in the football example. If you look at the chart above more closely, you’ll see that there are some other lines apart from the <font color=blue>blue</font> one. Actually, we can draw infinitely many lines in that graph that look more or less similar to the <font color=blue>blue line</font>. So, how does linear regression choose the best line among an infinite number of alternatives? Linear regression does this by using an optimization algorithm called **Ordinary Least Squares (OLS for short)**.\n",
    "\n",
    "OLS is a relatively simple algorithm (we’ll see it later). It tries to minimize the sum of the squared distances between each point and the line, and it chooses the line that minimizes this sum. This minimization is the optimization that the linear regression method does. After this optimization process, we get a function that is represented by the <font color=blue>blue line</font> in the chart. In the literature, this step is called **estimation**. \n",
    "  \n",
    "### 3. Assumptions of linear regression: \n",
    "\n",
    "After you estimated your model’s parameters, you need to make sure that some assumptions about your data and model hold. In particular, there are a small number of assumptions that must be met by your model in order to use linear regression. Linear regression assumes six conditions: \n",
    "\n",
    "  1. The relationship between feature(s) and target(s) is linear.\n",
    "  2. The errors of the model should be equal to zero on average.\n",
    "  3. The model’s errors are consistently distributed, which is known as **homoscedasticity**.\n",
    "  4. Features are at most only weakly correlated. Put differently there is not strong **multicollinearity**.\n",
    "  5. The model’s errors should be uncorrelated with each other.\n",
    "  6. The features and model errors are independent of one another.\n",
    "\n",
    "These assumptions probably sound abstract at the moment, but don’t worry. Later in this module, we’ll review these assumptions and also describe some approaches you can take to use linear regression even when your model doesn’t meet these conditions. Once you check for these conditions and try to fix your model if it doesn’t meet some of the conditions above, then you can move to the next step.\n",
    "  \n",
    "### 4. Understanding the relationship between features and target: \n",
    "\n",
    "After the previous steps, we get some values for the coefficients of the features that capture the relationship between the features and the target variable. We can quantify the relationship between the target variable and each feature using the estimated coefficients. Specifically, the estimated function that the <font color=blue>blue line</font> represents is:\n",
    "\n",
    "$$ average \\enspace number \\enspace of \\enspace goals = -0.01 + 0.09 \\enspace X \\enspace number \\enspace of \\enspace shots $$\n",
    "\n",
    "This implies that if the number of shots increases by 10, then the average number of goals increases by 0.9.\n",
    "\n",
    "\n",
    "### 5. Evaluating goodness of fit: \n",
    "\n",
    "To measure how well a model explains the information in the outcome variable, we look at the performance of the model on training data. The training data is comprised of the records that we use to estimate the model. As we'll see later in the bootcamp, to measure the performance of classification models, we use a confusion matrix to generate metrics like accuracy, precision, and recall.\n",
    "\n",
    "However, creating a confusion matrix won’t work for the continuous variables we encounter in regression problems — an infinite confusion matrix would be …. infinitely confusing. Instead, we use metrics like R-squared or adjusted R-squared to evaluate the goodness of fit of our model (more on these later in this module). If we are happy with the performance of our model, then we can proceed to the next steps. But, if we think that our model doesn’t explain the target variable very well, we need to change something in our model like adding some extra features and go back to step 2 (estimation phase).\n",
    "  \n",
    "### 6. Predicting the unknown: \n",
    "\n",
    "Regression models are quite flexible in predicting unknown target values. This is a consequence of how we estimate the relationship between features and target variable. Based on a finite set of observed cases, we generate a line that can, in principle, make a prediction about *any unobserved value*. By **prediction**, we mean to come up with outcome values for the observations that have not been previously observed. \n",
    "\n",
    "In the test set, we can look at the difference between the correct target value and what our model predicted it would be. When we think about prediction in this way, it gives us a way to objectively determine how well our model will perform on previously unseen observations. \n",
    "\n",
    "  ![and another chart depicting goals scored as function of goals attempted](assets/fantastic_football_linear_4.png)\n",
    "\n",
    "Let’s see how that works in practice by returning to our football example, where we created a model that estimated average number of goals as a function of number of shots. Furthermore, let’s imagine that we’ve been brought on to advise a football coach about the strategy he should undertake with his players — should they take more or fewer shots?\n",
    "\n",
    "As you can see in the chart above, it’s an unseen observation (the <font color=red>red point</font> in the graph) for the model we estimated. Now, we want to predict the average number of goals we can achieve if we succeed on our plan. Since we already estimated a relationship, we can do this by just feeding in the x-value of the observation (where the <font color=green>green vertical line</font> intersects with the x-axis) to the function we estimated. The result is the prediction (it is where the <font color=grey>grey horizontal line</font> intersects with the y-axis)!  So, we can expect to score almost 0.95 goals on average. If we have the information regarding the correct value of the average number of goals for the red point (if this point is from our test set, then we have this information), we could measure the success of our model's prediction which we'll talk about next.\n",
    "  \n",
    "### 7. Measuring test performance: \n",
    "\n",
    "So we have a function that predicts how many goals we can expect to be scored per match based on number of shots. But will this prediction be reliable in practice? Should our strategy rest upon that information? To determine this, we need to evaluate our model on a test set. The test set is data we hold back that the model was not trained on that we can use to see how well the model performs on new cases.\n",
    "\n",
    "As in the case of measuring training performance, we can not use a confusion matrix to assess test set performance of regression models.  Instead, we use metrics like Root Mean Squared Error (RMSE) to evaluate our models, which we’ll learn about later in this module.\n",
    "\n",
    "Once we get the test set performance of our model, we can judge whether it is satisfactory or not. If we think that our model is doing a great job, then we can rely on the predictions it generates, and we can use those predictions to direct our football team’s strategy. But, if we think that our model is not good enough, then we should go back to the step 2 (estimation phase) and try other model alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From simple to more complex linear regression models\n",
    "\n",
    "In this checkpoint, we illustrated the regression problem using a fictional dataset that only includes a feature and a target. However, in the real world, we usually work with datasets that include more than one feature. Thankfully, linear regression models can handle many features and even multiple targets! These kinds of linear models are called **multivariable** and **multivariate** respectively. Throughout this module, we illustrate the power of linear regression models using an insurance cost dataset that involves several features. \n",
    "\n",
    "Moreover, the term *“linear”* in the name of the linear regression model is there not to indicate that the fitted line is a linear one but instead to indicate that the linear relationship is between the target variable and the coefficients of the features. In fact, linear regression models can estimate relationships that are quadratic, cubic, or any polynomial order. In the rest of this module, we will demonstrate how we can estimate non-linear relationships between target(s) and feature(s) using linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "To close out this checkpoint, answer the following questions.  Once you're done, you can compare your answers to the ones found in [this notebook, which you can download](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/machine-learning-regression/solutions/1.solution_what_is_regression.ipynb).\n",
    "\n",
    "1. Let's assume that you have World Bank data on financial, economic and social indicators for several countries. You want to measure the factors that affect the level of development in these countries. To this end, you decide to use per capita income as a proxy for the development level, which is defined as the national income divided by the population. You want to use some features in your dataset to predict per capita income. Is this task a classification or a regression task? Why? \n",
    "2. Which of the following arguments are false and why?\n",
    "  * OLS is a special type of linear regression models \n",
    "  * Regression models become useless if they don’t meet the assumptions of linear regression. \n",
    "  * Estimation and prediction are the same thing in the context of linear regression models.\n",
    "  * Linear regression is only one kind of regression model. Regression problems can also be solved with other kind of models like Support Vector Machines or Random Forests.\n",
    "3. Assume that your project manager wants you to discover which free services your company offers make your customers buy more of your paid services. Formulate this task as a regression problem and write down the potential outcome and features that you’d like to work on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
